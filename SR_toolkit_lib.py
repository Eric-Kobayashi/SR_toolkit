'''
SR_toolkit_v3_0

Created: 12-Nov-2018 17:40
First published: 03-Dec-2018

@author: Eric Kobayashi (Based on DRW's SR_toolkit_v2)

Major changes over v2:
    1. Pack the functions in the v2 scripts into 3 classes and their methods:
        a. Analysis class - logs the analysis parameters, collects and saves the
        outputs, with the flexibility of packing more analysis in the class.
        A log file is saved to keep track of the parameters used in the analysis.
        
        b. SR_fit class - represents a fit result file aka. an image.
        Records the parameters passed by the analysis class, runs the 
        DBSCAN cluster and the subsequent characterisations of the clusters.
        
        c. cluster_track class - for each cluster identified in the cluster 
        analysis. Stores the characteristics of the cluster.
    
    2. Add functionalities: burst filter and burst analysis
       These methods analyse the time dimension of each cluster with a certain 
       fill_gap parameter to account for dye blinking.
       
       Burst filter (set with a higher fill_gap) tries to remove clusters that 
       have less than N bursts. This is assuming the real events are recurring 
       and the non-specific bindings are random, non-recurring. One could play 
       around the balance between min_burst and DBSCAN_min_samples options to 
       see how the results changes.
       
       Burst analysis (usually set with a lower fill_gap) measures the on and 
       off time of each cluster. This is related with qPAINT analysis, which 
       determines how many labelled molecules are at one spot. See Jungmann paper
       for details. The remove_single option
        
    3. Remove Nearest neighbour analysis: 
        
    The NN analysis made assumption such as defining 5 times nearest neighbour 
    distances as the 'neighbourhood'; the nearest neighbour distance is not really
    informative since the nearest neighbours usually come from the same 
    binding spanning in mulitple frames.
    
    Since the anlysis was intended to measure the density of localisations,
    convex hull area is added here so that the 2D density of the aggregates
    can be estimated. 
    
    More elegant methods, such as a 'Concave hull' (with a smoothing constant)
    or Ripley's k function could be added in the features.
    
Some minor changes:
    1. Remove the generate SR_mage, ThT maxima, cropping functionality. 
    The former two are more convenient done in imageJ, the latter will be added
    in a future version, cropping either space or time.
    
    2. The new analysis makes some uses of the Image.results.xls file which 
    records the frame number, width and height. However it will throw an error
    if the file does not exist. 
    
    3. Timestamp is used to log the analysis, giving each image a unique ID 
    which will be handy in SQL operations. A relationship can be easily built 
    between the summary and histograms file. 
    
    4. A folder will be generated to hold the analysis outputs everytime an 
    Analysis object is generated. There is also an option of copying all analysis 
    generated files to the results folder. This avoids overwriting the previous 
    results.
    
    5. More to cover...


Potential features update:
    1. Create symlink for raw images.
    2. Integrate ImageJ analysis including GDSC SMLM, background correction, 
    find maxima (ThT counting), fiducial correction into the main analysis.
    3. Doing DBSCAN on bursts (events lasting multiple frames count as one burst)
    rather than localisations can be more precise.
    4. Use burst analysis for fiducial correction.
    5. Length measurement is slow. Could be improved.
    6. Integrate sqlite to pack the outputs into database automatically.
    7. See minor changes 1, major changes 3.
    
'''


import os
import sys
from datetime import datetime
from collections import OrderedDict
from pandas import DataFrame as DF


from SR_fit_lib import SR_fit

# debug mode
import SR_fit_lib
import importlib
importlib.reload(SR_fit_lib)



class Analysis(object):
    '''
    This class is used to package the analysis together.
    
    '''
    def __init__(self, path):
        self.path = path
        timestring = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        self.timestring = timestring
        self.results_dir = os.path.join(path, 'Analysis_'+timestring)
        os.mkdir(self.results_dir)
        sys.stdout.write("Time: {} Analysis starts!\n".format(timestring))
        self.logstring = '''Super-resolution fitting analysis
Time: {}
Path: {}\n'''.format(timestring, path)
        
    def run_fit(self, fit_name=None, verbose=True, **kwargs):
        '''
        Run fit analysis on fitresults file generated by imageJ GDSC SMLM plugin.
        verbose set to False will supress all stdout logs but the start and finish
        times.
        
        Accepted keywords in kwargs:
            pixel_size, DBSCAN_eps, DBSCAN_min_samples, sr_scale=8, frame_length=50,
            burst_filter={'run':True, 'min_burst':2, 'fill_gap':50},
            burst_analysis={'run':True, 'fill_gap':10, 'remove_single':True},
            length_measure=True, eccentricity_measure=True, convexhull_measure=True,
            copy_analysis=False, save_histogram=True, save_GDSC_header_file = True
        '''
                
        if verbose:
            sys.stdout.write("Looking for super-res fit result files '{}' in \npath: {}\n".format(
                fit_name, self.path))  
        self._search_fitresults(fit_name)
        self.resultslist = [SR_fit(fitpath) for fitpath in self.fitpathlist]
        self._initialise_output(**kwargs)
        if verbose:
            sys.stdout.write("{} fit results found.\n\n".format(self.n))
            sys.stdout.write("Run cluster analysis:\n")             
            i = 1

        # Run cluster analysis
        for fit in self.resultslist:
            fit_ID = 'T'+ self.timestring + '_' + fit.path # This serves as the
                                    # primary key for the summary database
            fit.update_ID(fit_ID)
            self.to_summary['Analysis_ID'].append(fit.fit_ID)
            self.to_summary['Raw_loc_number'].append(fit.loc_num())
            
            # Copy files generated in the analysis to the results directory
            if kwargs['copy_analysis']:
                new_dir = fit.root.replace(self.path, self.results_dir)
                if not os.path.isdir(new_dir):
                    os.makedirs(new_dir)
                kwargs['copy_analysis'] = new_dir
                
            fit.input_parameters(**kwargs)
            fit.cluster_info()
            self.to_summary['Clustered_loc_number'].append(fit.loc_num())
            
            if verbose:
                sys.stdout.write('\r')
                sys.stdout.write("%d%%" % (i/self.n*100))
                sys.stdout.flush()
                i += 1
                
        if verbose:
            sys.stdout.write("\nRun burst filter:\n")             
            i = 1
        # Run burst filter if applied
        paras = kwargs['burst_filter'].copy()
        if paras.pop('run', False):
            for fit in self.resultslist:
                fit.burst_filter(**paras)
                self.to_summary['Filtered_cluster'].append(fit.filtered_cluster)
                
                if verbose:
                    sys.stdout.write('\r')
                    sys.stdout.write("%d%%" % (i/self.n*100))
                    sys.stdout.flush()
                    i += 1
                
        # Output cluster info after burst filter        
        for fit in self.resultslist:
            self.to_summary['Number_of_clusters'].append(fit.cluster_num())
            self.to_summary['Average_loc_per_cluster'].append(fit.ave_cluster_locnum())
            
            if kwargs['save_histogram']:
                self.to_hist_cluster['Analysis_ID'] += [fit.fit_ID]*fit.cluster_num()
                self.to_hist_cluster['Cluster_ID'] += fit.output_histogram('num')
                self.to_hist_cluster['Loc_number'] += fit.output_histogram('loc_num')
                
        # Run burst analysis if applied    
        if verbose:
            sys.stdout.write("\nRun burst analysis:\n")             
            i = 1  
        
        paras = kwargs['burst_analysis'].copy()
        if paras.pop('run', False):
            for fit in self.resultslist:
                fit.burst_info(**paras)

                self.to_summary['Average_burst_number'].append(fit.ave_burstnum)
                self.to_summary['Average_burst_length_(s)'].append(fit.ave_burstlen)
                self.to_summary['Average_dark_length_(s)'].append(fit.ave_darklen)
                self.to_summary['Average_lighttodark_ratio'].append(fit.ave_lighttodark)
                
                if kwargs['save_histogram']:
                    self.to_hist_cluster['Burst_number'] += fit.output_histogram('burstnum')
                    self.to_hist_cluster['Average_burst_length_(s)'] += fit.output_histogram('ave_burstlen')
                    self.to_hist_cluster['Average_dark_length_(s)'] += fit.output_histogram('ave_darklen')
                    
                    alllen = fit.output_noncluster_histogram('alllen')
                    self.to_hist_len['Analysis_ID'] += [fit.fit_ID]*len(alllen[0])
                    self.to_hist_len['Cluster_ID'] += alllen[0]
                    self.to_hist_len['All_burst_length_(s)'] += alllen[1]
                    
                    alldarklen = fit.output_noncluster_histogram('alldarklen')
                    self.to_hist_darklen['Analysis_ID'] += [fit_ID]*len(alldarklen[0])
                    self.to_hist_darklen['Cluster_ID'] += alldarklen[0]
                    self.to_hist_darklen['All_dark_length_(s)'] += alldarklen[1]
                
                if verbose:
                    sys.stdout.write('\r')
                    sys.stdout.write("%d%%" % (i/self.n*100))
                    sys.stdout.flush()
                    i += 1

        if verbose:
            sys.stdout.write("\nRun length measurment:\n")             
            i = 1            
        if kwargs['length_measure']:
            for fit in self.resultslist:
                fit.length_measure()
                self.to_summary['Average_length_(nm)'].append(fit.ave_length)
                self.to_summary['Average_density_1D'].append(fit.ave_density_1D)
                
                if kwargs['save_histogram']:
                    self.to_hist_cluster['Length_(nm)'] += fit.output_histogram('nm_length')
                    
                if verbose:
                    sys.stdout.write('\r')
                    sys.stdout.write("%d%%" % (i/self.n*100))
                    sys.stdout.flush()
                    i += 1
                    
        if verbose:
            sys.stdout.write("\nRun eccentricity measurment:\n")             
            i = 1      
        if kwargs['eccentricity_measure']:
            for fit in self.resultslist:
                fit.eccentricity_measure()
                self.to_summary['Average_eccentricity'].append(fit.ave_ecc)
                self.to_summary['Average_flattening'].append(fit.ave_flattening)
                
                if kwargs['save_histogram']:
                    self.to_hist_cluster['Eccentricity'] += fit.output_histogram('ecc')
                    self.to_hist_cluster['Flattening'] += fit.output_histogram('flattening')
                    
                if verbose:
                    sys.stdout.write('\r')
                    sys.stdout.write("%d%%" % (i/self.n*100))
                    sys.stdout.flush()
                    i += 1

        if verbose:
            sys.stdout.write("\nRun convex hull area measurment:\n")             
            i = 1           
        if kwargs['convexhull_measure']:
            for fit in self.resultslist:
                fit.convexhull_meausure()
                self.to_summary['Average_convexhull_area_(nm2)'].append(fit.ave_area)
                self.to_summary['Average_density_2D'].append(fit.ave_density_2D)
                
                if kwargs['save_histogram']:
                    self.to_hist_cluster['Convex_hull_area_(nm2)'] += fit.output_histogram('nm2_area')
                    
                if verbose:
                    sys.stdout.write('\r')
                    sys.stdout.write("%d%%" % (i/self.n*100))
                    sys.stdout.flush()
                    i += 1

        if verbose:
            sys.stdout.write("\nSaving results...")         
        if kwargs['save_GDSC_header_file']:
            for fit in self.resultslist:
                fit.save_with_header()
                
        self._save_summary()
                
        if kwargs['save_histogram']:
            self._save_histogram()
        
        sys.stdout.write("\nTime: {} Analysis finished!".format(
            datetime.now().strftime('%Y-%m-%d_%H-%M-%S')))
        
    def _search_fitresults(self, fit_name):
        List_of_fitresults = []
        for roots, dirs, files in os.walk(self.path):
            for f in files:
                if f == fit_name:
                    List_of_fitresults.append(os.path.join(roots, f))
        self.fitpathlist = List_of_fitresults
        self.n = len(List_of_fitresults)
        
    def _initialise_output(self, **kwargs):
        '''
        Write the analysis parameters into the logstring and initialise the 
        dictionaries for the saving of results.
        OrderedDefaultDict is used here. (defined below) Order is preserved so 
        the output dataframe columns follow the set order. Default method is used
        for dynamically adding new fields.
        '''
        
        self.logstring += '''\nCluster analysis and measurements:
Pixel size: {} nm
DBSCAN_epsilon: {} nm
DBSCAN_min_samples: {}
Super-res scale: {}
Frame length: {} ms
Fit Results file number: {}'''.format(kwargs['pixel_size'], 
            kwargs['DBSCAN_eps'], kwargs['DBSCAN_min_samples'], 
            kwargs['sr_scale'], kwargs['frame_length'], self.n)
               
        self.to_summary = OrderedDefaultDict(list) 
        self.to_hist_cluster = OrderedDefaultDict(list) 
        self.to_hist_len = OrderedDefaultDict(list) 
        self.to_hist_darklen = OrderedDefaultDict(list) 
        
        # Add optional headers
        paras = kwargs['burst_filter'].copy()
        if paras.pop('run', False):
            self.logstring += '''
Burst filter: ON
    parameters: {}'''.format(str(paras))
        else:
            self.logstring += "\nBurst filter: OFF"
            
        paras = kwargs['burst_analysis'].copy()
        if paras.pop('run', False):
            self.logstring += '''
Burst analysis: ON
    parameters: {}'''.format(str(paras))
        else:
            self.logstring += "\nBurst analysis: OFF"
        
        with open(os.path.join(self.results_dir, 'log.txt'), 'w') as log:
            log.write(self.logstring)
            
    def _save_summary(self):
        DF(self.to_summary).to_csv(os.path.join(self.results_dir, 'Summary.csv'),
            columns=self.to_summary.keys(), index=False)
            
    def _save_histogram(self):
        DF(self.to_hist_cluster).to_csv(os.path.join(self.results_dir, 'Histogram_clusters.csv'),
            columns=self.to_hist_cluster.keys(), index=False)
            
        DF(self.to_hist_len).to_csv(os.path.join(self.results_dir, 'Histogram_all_burstlen.csv'),
            columns=self.to_hist_len.keys(), index=False)
            
        DF(self.to_hist_darklen).to_csv(os.path.join(self.results_dir, 'Histogram_all_darklen.csv'),
            columns=self.to_hist_darklen.keys(), index=False)

class OrderedDefaultDict(OrderedDict):
    # Source: http://stackoverflow.com/a/6190500/562769
    
    def __init__(self, default_factory=None, *a, **kw):
        if (default_factory is not None and
           not hasattr(default_factory, '__call__')):
            raise TypeError('first argument must be callable')
        OrderedDict.__init__(self, *a, **kw)
        self.default_factory = default_factory

    def __getitem__(self, key):
        try:
            return OrderedDict.__getitem__(self, key)
        except KeyError:
            return self.__missing__(key)

    def __missing__(self, key):
        if self.default_factory is None:
            raise KeyError(key)
        self[key] = value = self.default_factory()
        return value

    def __reduce__(self):
        if self.default_factory is None:
            args = tuple()
        else:
            args = self.default_factory,
        return type(self), args, None, None, self.items()

    def copy(self):
        return self.__copy__()

    def __copy__(self):
        return type(self)(self.default_factory, self)

    def __deepcopy__(self, memo):
        import copy
        return type(self)(self.default_factory,
                          copy.deepcopy(self.items()))

    def __repr__(self):
        return 'OrderedDefaultDict(%s, %s)' % (self.default_factory,
                                               OrderedDict.__repr__(self))
                                               
if __name__ == '__main__':
    directory = r"C:\Users\yz520\Desktop\OneDrive - University Of Cambridge\igorplay\training_set2"
    fit_filename = 'FitResults_feu_removed.txt'
    
    input_dict = {
    'pixel_size': 97.7 , # nm
    'DBSCAN_eps': 100 , # nm, not pixels!
    'DBSCAN_min_samples': 10 ,
    'sr_scale': 8 , # The scale used in length analysis and 
                    # generation of super-res images 
    'frame_length': 50 , # ms, frame 
    'burst_filter': {'run':True, 'fill_gap':50, 'min_burst':2} ,
        # filter out the aggregates with less than min_bursts
    'burst_analysis': {'run':True, 'fill_gap':10, 'remove_single':True} ,
    
    'length_measure': True ,
    
    'eccentricity_measure': True ,
    
    'convexhull_measure': True , # Measure the area of the cluster
    
    'copy_analysis': True ,
    
    'save_GDSC_header_file': True ,
    
    'save_histogram':True}
    
    # Convert DBSCAN_eps from nm to pixels
    
    test = Analysis(directory)
    test.run_fit(fit_name=fit_filename, verbose=True, **input_dict)